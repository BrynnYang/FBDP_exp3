# 金融大数据-实验3

### 1、阶段一

> 任务（MapReduce）：
>
> 精简数据集：淘宝双十一用户购物数据集（100万条），见附件 million_user_log.csv.zip
>
> 基于精简数据集完成MapReduce作业：
>
> o   统计各省的双十一前十热门关注产品（“点击+添加购物车+购买+关注”总量最多前10的产品）
>
> o   统计各省的双十一前十热门销售产品（购买最多前10的产品）

#### 1.1、实验环境

- Win10

- hadoop-3.0.0

- pycharm

- 本地运行

我们将用Python为Hadoop编写一个简单的MapReduce程序，但是不使用Jython将代码转换成Java jar文件。

##### 参考教程:

1、如何用python编写mapreduce程序——以词频统计为例

https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/#test-your-code-cat-data--map--sort--reduce

#### 1.2、map.py

- key：省份
- value：item_id+action

```python
# coding=UTF-8
#!/usr/bin/env python
"""map.py"""

import sys
for row in sys.stdin:
    row = row.strip()
    users = line.split(',')
    print ('%s\t%s,%s' % (users[10],users[1], users[7]))//省份、商品id、行动action
```

#### 1.3 reducer.py

- 数据结构：字典dict={}
- dict[key]=[{关注},{销售}]，key值表示一个包含双字典的列表。

```python
# coding=UTF-8
# !/usr/bin/env python
"""reducer.py"""

from operator import itemgetter
import sys

key={}
# input comes from STDIN
for row in sys.stdin:
    row = row.strip()
    # from map.py
    province, value = row.split('\t')
    item_id, action = value.split(',')
    sales=int(action)
    if province not in key:
        key[province]=[{},{}]
        commodity_top10_1=key[province][0]
        commodity_top10_2=key[province][1]
        commodity_top10_1[item_id]=commodity_top10_1.get(item_id,0)+1
        if sales == 2:
            commodity_top10_2[item_id]=commodity_top10_2.get(item_id,0)+1
    else:
        commodity_top10_1=key[province][0]
        commodity_top10_2=key[province][1]
        commodity_top10_1[item_id]=commodity_top10_1.get(item_id,0)+1
        if sales == 2:
            commodity_top10_2[item_id]=commodity_top10_2.get(item_id,0)+1

for province in key:
    commodity_top10_1=key[province][0]
    commodity_top10_2=key[province][1]
    sort1 = sorted(commodity_top10_1.items(),key=lambda x: x[1], reverse=True)
    sort2 = sorted(commodity_top10_2.items(),key=lambda x: x[1], reverse=True)
    print(province+':'+'\n')
    print('前十热门关注产品:')
    for i in range(10):
        print(sort1[i], end=" ")
    print('\n')
    print('前十热门销售产品:')
    for k in range(10):
        print(sort2[k], end=" ")
    print('\n')
```

#### 1.4、运行结果

终端运行程序：

```python
type million_user_log.csv | python map.py | python reducer.py
```

运行结果见文件夹：阶段一运行结果截图。格式为：【省份+前世热门关注+前十热门销售】。

#### 1.5 反思

中途被输出时的省份乱码坑了好久。。。转成utf-8编码也没有用。后来找到一篇教程：《解决 Excel 打开 UTF-8 编码 CSV 文件乱码的 BUG》https://blog.csdn.net/leonzhouwei/article/details/8447643

这篇教程还是不错的，照着方法2做即可。



### 2、阶段二

#### 2.1、引言

一开始成功安装了hive。。。。。过程还比较艰辛，结果对于一些复杂的shell命令系统会崩溃，查了半天错也没法解决，最后使用了助教给的镜像，完成的很快。

这是一开始成功安装启动的截图。

##### 初始化hive

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\1、hive初始化成功.png)

##### 和mysql连接

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\2、hive和mysql成功连接.png)

之后就创建表格，导入数据之后就崩了，不管是网上说jar包冲突还是xml文件配置错误，都试过了n次。下面直接看镜像中的结果。

#### 2.2、熟悉的docker配方

##### docker中启动hive

先启动hadoop的进程，然后才能在bin目录下启动hive。

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\3、docker中启动hive.png)

###### 把精简数据集导入到数据仓库Hive中，并对数据仓库Hive中的数据进行查询分析

1、数据集命名为userlog，导入csv

```JAVA
hive> create table userlog(user_id int, item_id int, cat_id int, merchant_id int, brand_id int, month int, day int, action int,age_range int, gender int, province string)row format delimited fields terminated by ',' ;
//在这之前要把csv文件从本地系统拷贝到镜像容器中
brynnyang@ubuntu: sudo docker cp /home/brynnyang/million_user_log.csv h01:/

hive> load data local inpath '/million_user_log.csv' overwrite into table userlog;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\4、docker中创建表格+导入数据.png)

2、查看前20行数据

```Java
hive> select * from userlog limit 20;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\5、查看前20条数据.png)

3、发现异常数据

其实一打开csv就能发现在最前面几行，同一user存在性别不一、年龄范围不一的情况，因此需要对数据进行预处理。

```java
hive> select * from userlog where id=328862 and item_id=406349;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\6、数据清洗前-异常数据.png)

##### 数据清洗

1、建立临时表格，记录异常（user_id对应的年龄、性别不一）的数目；

```java
hive> create table tmp as select user_id, count(distinct gender) as gender_cot,
count(distinct age_range) as age_cot from userlog group by user_id;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\7、数据清洗-建立临时表格.png)

2、清洗完成，建立新的update版表格，把刚刚count=1的非异常数据导入。

```java
hive> create table userlog_update as select * from userlog where user_id in (select user_id from tmp where (gender_cot=1 and age_cot=1));
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\8、数据清洗完成后的表格.png)

##### 问题一：查询双11那天有多少人购买了商品

购买-action=2

```java
hive> select count(distinct user_id) from userlog_update where action=2;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\9、Q1-查看购买量.png)

##### 问题二：查询双11那天男女买家购买商品的比例

这一问有点歧义，不知道是具体算出男性和女性的比例；还是算一个男性/女性的比值，我选了前者来做。

```java
hive> select (select count(distinct user_id) from userlog_update where action=2 and gender=1//0)/(select count(distinct user_id) from cleaned_userlog where action=2);
//男性gender=1，女性=0`
```

男性比例：0.329

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\10、Q2-男性购买比例.png)

女性比例：0.334

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\11、Q2-女性购买比例.png)



##### 问题三：查询双11那天浏览次数前十的品牌

按照品牌分组，对每个品牌计算人数，最后按照降序排列取前十。

```java
hive> select brand_id,count(brand_id) number from userlog_update where action=0 group by brand_id order by number desc limit 10;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段二运行截图\阶段二运行截图\12、Q3-查看浏览量前十.png)

##### 参考资料：

1、基于docker搭建hadoop平台

https://zhuanlan.zhihu.com/p/59758201

2、ubuntu16.04安装hive-2.3.3

https://www.jianshu.com/p/c1eefd03b850

3、启动hive报错：java.lang.NoSuchMethodError。。。

http://www.bubuko.com/infodetail-3286965.html

4、ubuntu安装hive，并配置mysql作为元数据库

http://dblab.xmu.edu.cn/blog/install-hive/

