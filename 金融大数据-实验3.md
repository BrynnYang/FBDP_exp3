# 金融大数据-实验3

### 1、阶段一

> 任务（MapReduce）：
>
> 精简数据集：淘宝双十一用户购物数据集（100万条），见附件 million_user_log.csv.zip
>
> 基于精简数据集完成MapReduce作业：
>
> o   统计各省的双十一前十热门关注产品（“点击+添加购物车+购买+关注”总量最多前10的产品）
>
> o   统计各省的双十一前十热门销售产品（购买最多前10的产品）

#### 1.1、实验环境

- Win10

- hadoop-3.0.0

- pycharm

- 本地运行

我们将用Python为Hadoop编写一个简单的MapReduce程序，但是不使用Jython将代码转换成Java jar文件。

##### 参考教程:

1、如何用python编写mapreduce程序——以词频统计为例

https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/#test-your-code-cat-data--map--sort--reduce

#### 1.2、map.py

- key：省份
- value：item_id+action

```python
# coding=UTF-8
#!/usr/bin/env python
"""map.py"""

import sys
for row in sys.stdin:
    row = row.strip()
    users = line.split(',')
    print ('%s\t%s,%s' % (users[10],users[1], users[7]))//省份、商品id、行动action
```

#### 1.3 reducer.py

- 数据结构：字典dict={}
- dict[key]=[{关注},{销售}]，key值表示一个包含双字典的列表。

```python
# coding=UTF-8
# !/usr/bin/env python
"""reducer.py"""

from operator import itemgetter
import sys

key={}
# input comes from STDIN
for row in sys.stdin:
    row = row.strip()
    # from map.py
    province, value = row.split('\t')
    item_id, action = value.split(',')
    sales=int(action)
    if province not in key:
        key[province]=[{},{}]
        commodity_top10_1=key[province][0]
        commodity_top10_2=key[province][1]
        commodity_top10_1[item_id]=commodity_top10_1.get(item_id,0)+1
        if sales == 2:
            commodity_top10_2[item_id]=commodity_top10_2.get(item_id,0)+1
    else:
        commodity_top10_1=key[province][0]
        commodity_top10_2=key[province][1]
        commodity_top10_1[item_id]=commodity_top10_1.get(item_id,0)+1
        if sales == 2:
            commodity_top10_2[item_id]=commodity_top10_2.get(item_id,0)+1

for province in key:
    commodity_top10_1=key[province][0]
    commodity_top10_2=key[province][1]
    sort1 = sorted(commodity_top10_1.items(),key=lambda x: x[1], reverse=True)
    sort2 = sorted(commodity_top10_2.items(),key=lambda x: x[1], reverse=True)
    print(province+':'+'\n')
    print('前十热门关注产品:')
    for i in range(10):
        print(sort1[i], end=" ")
    print('\n')
    print('前十热门销售产品:')
    for k in range(10):
        print(sort2[k], end=" ")
    print('\n')
```

#### 1.4、运行结果

终端运行程序：

```python
type million_user_log.csv | python map.py | python reducer.py
```

运行结果见文件夹：阶段一运行结果截图。格式为：【省份+前世热门关注+前十热门销售】。

#### 1.5 反思

中途被输出时的省份乱码坑了好久。。。转成utf-8编码也没有用。后来找到一篇教程：《解决 Excel 打开 UTF-8 编码 CSV 文件乱码的 BUG》https://blog.csdn.net/leonzhouwei/article/details/8447643

这篇教程还是不错的，照着方法2做即可。



### 2、阶段二

#### 2.1、引言

一开始成功安装了hive。。。。。过程还比较艰辛，结果对于一些复杂的shell命令系统会崩溃，查了半天错也没法解决，最后使用了助教给的镜像，完成的很快。

这是一开始成功安装启动的截图。

##### 初始化hive

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\1、hive初始化成功.png)

##### 和mysql连接

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\2、hive和mysql成功连接.png)

之后就创建表格，导入数据之后就崩了，不管是网上说jar包冲突还是xml文件配置错误，都试过了n次。下面直接看镜像中的结果。

#### 2.2、熟悉的docker配方

##### docker中启动hive

先启动hadoop的进程，然后才能在bin目录下启动hive。

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\3、docker中启动hive.png)

###### 把精简数据集导入到数据仓库Hive中，并对数据仓库Hive中的数据进行查询分析

1、数据集命名为userlog，导入csv

```JAVA
hive> create table userlog(user_id int, item_id int, cat_id int, merchant_id int, brand_id int, month int, day int, action int,age_range int, gender int, province string)row format delimited fields terminated by ',' ;
//在这之前要把csv文件从本地系统拷贝到镜像容器中
brynnyang@ubuntu: sudo docker cp /home/brynnyang/million_user_log.csv h01:/

hive> load data local inpath '/million_user_log.csv' overwrite into table userlog;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\4、docker中创建表格+导入数据.png)

2、查看前20行数据

```Java
hive> select * from userlog limit 20;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\5、查看前20条数据.png)

3、发现异常数据

其实一打开csv就能发现在最前面几行，同一user存在性别不一、年龄范围不一的情况，因此需要对数据进行预处理。

```java
hive> select * from userlog where id=328862 and item_id=406349;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\6、数据清洗前-异常数据.png)

##### 数据清洗

1、建立临时表格，记录异常（user_id对应的年龄、性别不一）的数目；

```java
hive> create table tmp as select user_id, count(distinct gender) as gender_cot,
count(distinct age_range) as age_cot from userlog group by user_id;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\7、数据清洗-建立临时表格.png)

2、清洗完成，建立新的update版表格，把刚刚count=1的非异常数据导入。

```java
hive> create table userlog_update as select * from userlog where user_id in (select user_id from tmp where (gender_cot=1 and age_cot=1));
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\8、数据清洗完成后的表格.png)

##### 问题一：查询双11那天有多少人购买了商品

购买-action=2

```java
hive> select count(distinct user_id) from userlog_update where action=2;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\9、Q1-查看购买量.png)

##### 问题二：查询双11那天男女买家购买商品的比例

这一问有点歧义，不知道是具体算出男性和女性的比例；还是算一个男性/女性的比值，我选了前者来做。

```java
hive> select (select count(distinct user_id) from userlog_update where action=2 and gender=1//0)/(select count(distinct user_id) from cleaned_userlog where action=2);
//男性gender=1，女性=0`
```

男性比例：0.329

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\10、Q2-男性购买比例.png)

女性比例：0.334

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\11、Q2-女性购买比例.png)



##### 问题三：查询双11那天浏览次数前十的品牌

按照品牌分组，对每个品牌计算人数，最后按照降序排列取前十。

```java
hive> select brand_id,count(brand_id) number from userlog_update where action=0 group by brand_id order by number desc limit 10;
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\阶段二运行截图\12、Q3-查看浏览量前十.png)

##### 参考资料：

1、基于docker搭建hadoop平台

https://zhuanlan.zhihu.com/p/59758201

2、ubuntu16.04安装hive-2.3.3

https://www.jianshu.com/p/c1eefd03b850

3、启动hive报错：java.lang.NoSuchMethodError。。。

http://www.bubuko.com/infodetail-3286965.html

4、ubuntu安装hive，并配置mysql作为元数据库

http://dblab.xmu.edu.cn/blog/install-hive/



### 3、阶段三

运行环境：win10、spark2.4.4

语言：python+sparkSQL

IDE：pycharm

#### 3.1 安装spark

直接在官网下载，解压到D盘（将下载的文件解压到一个目录，注意目录不能有空格）

https://www.apache.org/dyn/closer.lua/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz

将spark的bin路径添加到path中

cmd命令行运行spark-shell，发现spark安装成功

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\spark成功安装.png)

由于用python编写，需要另外安装pyspark。

```
pip install pyspark -i https://pypi.tuna.tsinghua.edu.cn/simple
```

在cmd中输入pyspark，查看Pyspark版本信息

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\pyspark安装.png)

**运行自带的Spark example测试程序**

打开cmd，输入spark-submit --class org.apache.spark.examples.SparkPi --master local [spark exmple 路径]

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\测试spark自带示例.png)

#### 3.2 任务1

> 统计各省销售最好的产品类别前十（销售最多前10的产品类别）

首先读取数据，得到初始的RDD，执行.cache()；过滤数据集，筛选出销售出的商品类别；之后改变(key, value)，key表示为省份和商品类别，求和。最后将省份作为key，用value表示商品类别、商品id和购买次数的集合，按照购买次数的降序排序，输出前十位。

结果如下：

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\3_1结果1.png)

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\3_1结果2.png)

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\3_1结果3.png)

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\3_1结果4.png)

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\3_1结果5.png)



#### 3.3 任务2

> 统计各省的双十一前十热门销售产品（购买最多前10的产品）

和前一问的思路完全一致，但是key值压缩的时候把商品类别换成商品id即可。

部分结果如下，完整截图见截图文件夹：

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\3_2结果1.png)

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\3_2结果2.png)

 和MapReduce作业对比结果：

以安徽为例，分别看上图的第二张图的第一条数据，和下图的第一条数据（前十热门销售产品），对比发现结果一致。

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段1\阶段一运行结果截图\阶段1-结果3.png)

##### 总结

Spark环境下代码运行速度非常快，python编写程序也更简洁，但感觉网上教程不是很多诶。附一些参考教程如下：

1、Mac下安装spark，并配置pycharm-pyspark完整教程

https://blog.csdn.net/shiyutianming/article/details/99946797

2、解决Python用pip命令安装速度慢，改用国内镜像

https://blog.csdn.net/shawroad88/article/details/87692142

3、核心概念之SparkContext

https://www.missshi.cn/api/view/blog/5cd124d7cb996370fe000000

4、reduceByKey应用举例

https://www.jianshu.com/p/af175e66ce99

5、windows下搭建spark+python 开发环境

https://www.cnblogs.com/bradwarden/p/10940895.html

#### 3.4 任务3

> 查询双11那天浏览次数前十的品牌 -- 和Hive作业对比结果

读完题发现第三个问题在hive作业里也出现过，不用分省份（看起来比较简单），因此尝试使用sparkSQL，直接通过数据查询完成这一问，结果发现非常方便！！！

不过需要注意的是csv文件需要自己提前加一行小标题/scheme，不然写SQL语句也不方便。

```python
select brand_id,count(brand_id) number from dataset where action=0 group by brand_id order by number desc limit 10
```

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段3\阶段三截图\3_3结果.png)

对比hive作业的结果（未清洗数据前的）

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段2\原始没清洗的.png)

一模一样！

至此完成阶段三，可以看到spark环境下的运行结果和hive、mapreduce都是一致的，就SparkSQL和hive而言，两者速度接近，sparkSQL也是一种非常简便的表达方式。spark比mapreduce的运行速度真的快很多，具体原因在这里做一个温习吧：

> spark通过借鉴Hadoop mapreduce发展而来，继承了其分布式并行计算的优点，并改进了mapreduce明显的缺陷，具体表现在以下几方面：
>
> 1.**spark把中间计算结果存放在内存中，减少迭代过程中的数据落地，能够实现数据高效共享，迭代运算效率高。**mapreduce中的计算中间结果是保存在磁盘上的，这样必然影响整体运行速度。
>
> 2.**spark容错性高。**spark支持DAG图的分布式并行计算（简单介绍以下spark DAG：即有向无环图，描述了任务间的先后依赖关系，spark中rdd经过若干次transform操作，由于transform操作是lazy的，因此，当rdd进行action操作时，rdd间的转换关系也会被提交上去，得到rdd内部的依赖关系，进而根据依赖，划分出不同的stage。），它引进rdd弹性分布式数据集的概念，它是分布在一组节点中的只读对象集合，如果数据集一部分数据丢失，则可以根据血统来对它们进行重建；另外在RDD计算时可以通过checkpoint来实现容错，checkpoint有两种方式，即checkpiont data 和logging the updates。
>
> 3.**spark更加通用。**hadoop只提供了map和reduce两种操作，spark提供的操作类型有很多，大致分为转换和行动操作两大类。转换操作包括：map, filter, flatmap, sample, groupbykey, reducebykey, union, join, cogroup, mapvalues, sort, partitionby等多种操作，行动操作包括：collect, reduce, lookup和save等操作。

##### 参考资料：

1、spark与mapreduce的区别

https://www.cnblogs.com/db-record/p/11405159.html

2、Spark对比MapReduce究竟提高了多少效率？

https://blog.csdn.net/weixin_44233163/article/details/88825162

3、SparkSQL读取CSV文件

https://blog.csdn.net/qq_37004052/article/details/83276261



### 4、阶段4

> 数据挖掘：
>
> ·    针对预处理后的训练集和测试集，基于MapReduce或Spark MLlib编写程序预测回头客
>
> 2、评估预测准确率（可选）

一共采用了四种模型：SVM、RandomForest、DecisionTree和Logistics。基于mllib库，由于数据集本身存在一定问题，预测结果并不好（全是0），但几个模型的准确率相对来说还是比较高的。代码参考了官方文档，在此基础上进行了修改。

#### 4.1 SVM

线性支持向量机是大规模分类任务的标准方法。如式(1)所示，是一种线性方法，其损失函数如下：
$$
L(w;x,y):=max{0,1−ywTx}.
$$
默认情况下，线性支持向量机是用L2正则化训练的。我们也支持L1正则化。在这种情况下，问题变成了一个线性程序。线性支持向量机算法输出一个支持向量机模型。给定一个新的数据点，用x表示，模型根据wTx的值进行预测。默认情况下，如果wTx≥0，则结果为正，否则为负。

部分代码如下：

```python
...
data = sc.textFile("train_after.csv")
parsedData = data.map(parsePoint)
# split train and test
train_data,test_data=parsedData.randomSplit([0.7,0.3])
# Build the model
model = SVMWithSGD.train(parsedData, iterations=100)
...
```

##### 测试结果

准确率为**0.9432**，召回率为0

我们展示前20行的预测结果，会发现预测结果均为0……可能是数据集本身的问题导致，可以看出准确率还是可观的，算法效果不错。

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段4\阶段4截图\SVM结果.png)

#### 4.2 随机森林

随机森林是常用的分类和回归方法，它有如下优点：

1. 在当前所有算法中，具有极好的准确率/It is unexcelled in accuracy among current algorithms；
2. 能够有效地运行在大数据集上/It runs efficiently on large data bases；
3. 能够处理具有高维特征的输入样本，而且不需要降维/It can handle thousands of input variables without variable deletion；
4. 能够评估各个特征在分类问题上的重要性/It gives estimates of what variables are important in the classification；
5. 在生成过程中，能够获取到内部生成误差的一种无偏估计/It generates an internal unbiased estimate of the generalization error as the forest building progresses；
6. 对于缺省值问题也能够获得很好得结果/It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing

部分代码（使用模型部分）：

```python
#  Setting featureSubsetStrategy="auto" lets the algorithm choose.
model = RandomForest.trainClassifier(train_data, numClasses=2, categoricalFeaturesInfo={},
                                     numTrees=3, featureSubsetStrategy="auto",
                                     impurity='gini', maxDepth=4, maxBins=32)
```

这里，预测准确率为0.9409，预测结果均为0（均不是回头客）。虽然预测准确率不如SVM，但也很高，而且运行速度也较快。

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段4\阶段4截图\随机森林结果.png)

#### 4.3 决策树

部分代码：

```python
# Train a DecisionTree model.
#  Empty categoricalFeaturesInfo indicates all features are continuous.
model = DecisionTree.trainClassifier(train_data, numClasses=2, categoricalFeaturesInfo={}, impurity='gini', maxDepth=5, maxBins=32)
```

模型特点：

- 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
- 缺点：可能会产生过度匹配的问题
- 适用数据类型：数值型和标称型

预测准确率为0.9421，预测结果均为0（均不是回头客）。

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段4\阶段4截图\决策树结果.png)

#### 4.4 Logistics

部分代码：

```python
# Build the model
model = LogisticRegressionWithLBFGS.train(parsedData)
```

预测准确率为0.9432，预测结果均为0（均不是回头客）。和SVM一致，可能是因为都使用了线性分类回归（在官方文档中的方法原理一致/Linear Support）。

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段4\阶段4截图\logistics回归结果.png)



#### 4.5 四种方法对比

|            | SVM    | 随机森林 | 决策树 | Logistics |
| ---------- | ------ | -------- | ------ | --------- |
| **准确率** | 0.9432 | 0.9409   | 0.9421 | 0.9432    |
| **用时**   | 较快   | 较快     | 较慢   | 较慢      |

准确率对比图像如下：

![](E:\大三上\大数据（1+3+6）\实验\171870007 杨帆 实验3\阶段4\阶段4截图\不同模型的准确率比较.png)

#### 4.6 结论

综合运行速度和计算准确率，SVM算法效果相对而言是最佳的。测试结果为：均不是回头客（都是0），模型不存在问题，可能是数据集的问题。

python语言较为简洁，但需仔细学习官方文档中的各种函数和接口。

阶段4参考资料（官方文档）：

http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms

